Implementations:

file upload handling, text extraction, NLP (Natural Language Processing)

| Feature                   | Subtasks                                                                    |
| ------------------------- | --------------------------------------------------------------------------- |
| **1. Summary Generation** | File Upload → Extract Text → Summarize Each → Combine Summaries             |
| **2. Similarity Check**   | File Upload → Extract Text → Pairwise Text Comparison → Report Similarities |


Why Streamlit?
We focused on Backend rather than FrontEnd
Easy to Deploy with Streamlit Cloud


Our Flow:
1. Install Necessary Libraries
2. Model Selection & GPU Consideration
For the summarization task:
	Model: Use distilBART or distilT5. Transformers have a max token limit (~1024 for distilbart)
	Currently making use of facebook/bart-large-cnn: model downloaded locally
For similarity checking:
	Use sentence embeddings from sentence-transformers or HuggingFace models like distilbert-base-nli-stsb-mean-tokens.
3. Building the App with Streamlit
4. Running the App
5. Deployment


Explanation of Libraries:

transformers:
This is a library developed by Hugging Face, which provides access to pre-trained models for various natural language processing (NLP) tasks, such as summarization, translation, text generation, and more.
It supports a wide range of models like BERT, GPT, T5, BART, etc.

sentence-transformers:
This library is specifically designed to convert sentences or paragraphs into numerical vectors (embeddings), which can be used to measure the similarity between text data. The SentenceTransformer models are optimized to generate embeddings that represent the semantic meaning of the sentences, making them useful for similarity comparison.
It is commonly used for tasks like calculating text similarity using cosine similarity.


Cosine similarity is a measure of similarity between two non-zero vectors (in this case, the embeddings of text summaries). It measures the cosine of the angle between them, which can range from -1 (completely dissimilar) to 1 (completely similar).
	Why it works for text:
In text analysis, we convert each document or sentence into a vector using techniques like word embeddings or sentence embeddings (e.g., generated by SentenceTransformer).
Cosine similarity allows us to compare these vectors and determine how similar or different the text is by measuring the "angle" between the vectors, which reflects the semantic similarity between the texts.
For example:
If two summaries have a cosine similarity of 1, it means the summaries are almost identical.
If the similarity is 0, it means there is no similarity at all.





Summary Key Improvements:
Better Text Preparation:
Normalizes whitespace and sentence boundaries
Handles common formatting issues

Smarter Chunking:
First tries to preserve natural paragraphs
Falls back to sentence-based chunking only when needed
Maintains document structure better

Progressive Summarization:
Builds summary incrementally
Maintains context between chunks
Adjusts length dynamically

More Conservative Length Control:
Starts with 66% reduction instead of 75%
Adjusts for accumulated summary length
Better minimum length handling



Unified Caching System:
Single cache directory for both summaries and embeddings
Consistent naming convention ({file_hash}_{data_type}.pkl)
Error handling for cache operations

Efficient File Processing:
process_file() function handles both text extraction and caching
Returns file hash for tracking
Reads file bytes only once

Cache Awareness:
Visual feedback when using cached data (toast notifications)
Separate cache files for summaries and embeddings
Proper file handle management


Why not just make a simple API call to an online summarization service like OpenAI or Hugging Face’s hosted models?
We all know that the aim of prj is Learning
Also:
1. Cost Efficiency
2. Data Privacy and Security
3. Handling Long Documents (Beyond API Limits)
4. Offline Capability
5. Batch Processing
5. Customizability & Control
